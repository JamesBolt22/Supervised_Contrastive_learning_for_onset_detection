{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create_training_and_test_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLgTbhaCyKYraoaEYysqAY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesBolt22/Supervised_Contrastive_learning_for_onset_detection/blob/main/Create_training_and_test_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some code has been taken from https://github.com/rohitma38/cnn-onset-detection and adapted for use in this project."
      ],
      "metadata": {
        "id": "wZIYrOVdqI98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Libraries\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import math\n",
        "import h5py\n",
        "import keras.backend as K\n",
        "import glob"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KZ-nZM6dpPuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rvRAQtDKpJgG"
      },
      "outputs": [],
      "source": [
        "#@title Load all data files\n",
        "datadir='/content/drive/MyDrive/Processed_data_2/'\n",
        "songlist=np.loadtxt('/content/drive/MyDrive/SongNameList.txt',dtype=str)\n",
        "labels = np.load('/content/labels_master_2.npy', allow_pickle=True).item()\n",
        "weights = np.load('/content/drive/MyDrive/Final_data/weights_master.npy', allow_pickle=True).item()\n",
        "all_data_dict = np.load('/content/drive/MyDrive/Final_data/all_data_dict.npy', allow_pickle=True).item()\n",
        "\n",
        "#Only load partition data if created\n",
        "#partition = np.load('/content/drive/MyDrive/data_fold_0/split.npy', allow_pickle=True).item()\n",
        "\n",
        "with h5py.File('/content/drive/MyDrive/Final_data/all_data.h5', 'r') as hf:\n",
        "    all_data = hf[\"input_data\"][:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split data set\n",
        "partition = {'train':[], 'validation':[], 'test':[]}\n",
        "\n",
        "#define the network inputs which are in val_split and which are in test_split\n",
        "val_split = np.loadtxt(\"/content/drive/MyDrive/onsets_splits/onsets/splits/8-fold_cv_random_5.fold\",dtype='str')\n",
        "test_split = np.loadtxt(\"/content/drive/MyDrive/onsets_splits/onsets/splits/8-fold_cv_random_6.fold\",dtype='str')\n",
        "\n",
        "#runs through each song\n",
        "for song in songlist:\n",
        "  print(song)\n",
        "\n",
        "  #produces a list of inputs files\n",
        "  ids = glob.glob(datadir+song+'/*.pt.npy')\n",
        "\n",
        "  #adds file to correct partition\n",
        "  if song in val_split:\n",
        "    partition['validation'].extend(ids)\n",
        "  if song in test_split:\n",
        "    partition['test'].extend(ids)\n",
        "  else:\n",
        "    partition['train'].extend(ids)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KmQhSq_DpSTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save split\n",
        "np.save(\"/content/drive/MyDrive/data_fold_7/split\", partition)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OkfpoGHapXvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create training data from split\n",
        "\n",
        "#sets up the data arrays\n",
        "train_data = np.empty((len(partition['train']), 80,15,3))\n",
        "train_label = np.empty((len(partition['train']), 1))\n",
        "\n",
        "#runs through the train partition\n",
        "for i in range(len(partition['train'])):\n",
        "\n",
        "      # Select sample\n",
        "      ID = partition['train'][i]\n",
        "\n",
        "      #Load data and get label\n",
        "      idx = all_data_dict[ID[:-4]]\n",
        "      img = all_data[idx]\n",
        "      label = labels[ID[:-4]]\n",
        "\n",
        "      #add data to relevant array\n",
        "      train_data[i] = img\n",
        "      train_label[i] = label\n",
        "\n",
        "      print(i/len(partition['train'])*100)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MOa_U-W5pcXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create validation data from split\n",
        "\n",
        "#same process as above\n",
        "val_data = np.empty((len(partition['validation']), 80,15,3))\n",
        "val_label = np.empty((len(partition['validation']), 1))\n",
        "\n",
        "for i in range(len(partition['validation'])):\n",
        "\n",
        "      # Select sample\n",
        "      ID = partition['validation'][i]\n",
        "\n",
        "    # Load data and get label\n",
        "      idx = all_data_dict[ID[:-4]]\n",
        "      #img = all_data[idx]\n",
        "      \n",
        "      label = labels[ID[:-4]]\n",
        "      #val_data[i] = img\n",
        "      val_label[i] = label\n",
        "      print(i/len(partition['validation'])*100)\n",
        "    "
      ],
      "metadata": {
        "cellView": "form",
        "id": "NVbe4UeUpel2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create weights from data split\n",
        "\n",
        "#same process as above\n",
        "train_weights = np.empty((len(partition['train']),1))\n",
        "\n",
        "for i in range(len(partition['train'])):\n",
        "\n",
        "      # Select sample\n",
        "      ID = partition['train'][i]\n",
        "\n",
        "      # Load data and get label\n",
        "      temp = weights[ID[:-4]]\n",
        "      train_weights[i] = temp\n",
        "      print(i/len(partition['train'])*100)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eXmZ2UyFpgkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save weights\n",
        "weights_save = \"/content/drive/MyDrive/data_fold_0/weights_data_0.020.h5\"\n",
        "\n",
        "with h5py.File(weights_save, 'w') as hf:\n",
        "    hf.create_dataset(\"input_data\",  data=train_weights)\n",
        "    hf.close"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G-c5tI1Mpi7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save train and val data\n",
        "val_save = \"/content/drive/MyDrive/data_fold_0/val_data.h5\"\n",
        "val_label_save = \"/content/drive/MyDrive/data_fold_0/val_label_2.h5\"\n",
        "train_data_save = \"/content/drive/MyDrive/Sythn_data/created_data/train_data.h5\"\n",
        "train_labels_save = \"/content/drive/MyDrive/data_fold_0/train_labels_0.020.h5\"\n",
        "\n",
        "with h5py.File(val_save, 'w') as hf:\n",
        "    hf.create_dataset(\"input_data\",  data=val_data)\n",
        "    hf.close\n",
        "\n",
        "with h5py.File(val_label_save, 'w') as hf:\n",
        "    hf.create_dataset(\"input_data\",  data=val_label)\n",
        "    hf.close\n",
        "\n",
        "with h5py.File(train_data_save, 'w') as hf:\n",
        "    hf.create_dataset(\"input_data\",  data=train_data)\n",
        "    hf.close\n",
        "\n",
        "with h5py.File(train_labels_save, 'w') as hf:\n",
        "    hf.create_dataset(\"input_data\",  data=train_label)\n",
        "    hf.close"
      ],
      "metadata": {
        "cellView": "form",
        "id": "V7plOsQ5poqU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}