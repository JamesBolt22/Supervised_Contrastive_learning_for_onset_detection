{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProcessData.ipynb",
      "provenance": [],
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "12wdRgsQP90iKZY7ZA1Pw0_Dz8MC_8DPV",
      "authorship_tag": "ABX9TyPCfPpZdZYvSBbzXKTpIMnB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesBolt22/Supervised_Contrastive_learning_for_onset_detection/blob/main/ProcessData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code was taken from https://github.com/rohitma38/cnn-onset-detection with small adaptations to produce inputs for a onset detection CNN. "
      ],
      "metadata": {
        "id": "-MyF5Bixj6X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import h5py"
      ],
      "metadata": {
        "id": "Q9s3pPoPw7Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDTos8DjihqC"
      },
      "outputs": [],
      "source": [
        "#function to zero pad ends of spectrogram\n",
        "def zeropad2d(x,n_frames):\n",
        "\ty=np.hstack((np.zeros([x.shape[0],n_frames]), x))\n",
        "\ty=np.hstack((y,np.zeros([x.shape[0],n_frames])))\n",
        "\treturn y\n",
        "\n",
        "#function to create N-frame overlapping chunks of the full audio spectrogram  \n",
        "def makechunks(x,duration):\n",
        "\ty=np.zeros([x.shape[1],x.shape[0],duration])\n",
        "\tfor i_frame in range(x.shape[1]-duration):\n",
        "\t\ty[i_frame]=x[:,i_frame:i_frame+duration]\n",
        "\treturn y\n",
        "\n",
        "#data dirs\n",
        "audio_dir='/content/drive/MyDrive/onsets_audio/onsets/audio'\n",
        "onset_dir='/content/drive/MyDrive/onsets_annotations/onsets/annotations/onsets'\n",
        "save_dir='/content/drive/MyDrive/Processed_data_2'\n",
        "\n",
        "#data stats for normalization\n",
        "stats=np.load('/content/drive/MyDrive/means_stds.npy')\n",
        "means=stats[0]\n",
        "stds=stats[1]\n",
        "\n",
        "#context parameters\n",
        "contextlen=7 #+- frames\n",
        "duration=2*contextlen+1\n",
        "\n",
        "#load a list of song names for processing\n",
        "songlist=np.loadtxt('/content/drive/MyDrive/SongNameList.txt',dtype=str)\n",
        "\n",
        "#audio format type\n",
        "audio_format='.wav'\n",
        "\n",
        "#initialisation for specific lists\n",
        "labels_master={}\n",
        "weights_master={}\n",
        "filelist=[]\n",
        "\n",
        "#all_data contains all the created inputs, the number of data points is set up for the synthesized data set\n",
        "number_of_data_points = 51788 \n",
        "all_data = np.empty((number_of_data_points, 80,15,3))\n",
        "all_data_dict = {}\n",
        "idx = 0\n",
        "\n",
        "#runs through each song\n",
        "for item in songlist:\n",
        "  print(item)\n",
        "\n",
        "  #load audio and onsets\n",
        "  x,fs=librosa.load(os.path.join(audio_dir,item+audio_format), sr=44100)\n",
        "  if not os.path.exists(os.path.join(onset_dir,item+'.onsets')): continue\n",
        "  onsets=np.loadtxt(os.path.join(onset_dir,item+'.onsets'))\n",
        "\n",
        "  #get mel spectrogram\n",
        "  melgram1=librosa.feature.melspectrogram(x,sr=fs,n_fft=1024, hop_length=441,n_mels=80, fmin=27.5, fmax=16000)\n",
        "  melgram2=librosa.feature.melspectrogram(x,sr=fs,n_fft=2048, hop_length=441,n_mels=80, fmin=27.5, fmax=16000)\n",
        "  melgram3=librosa.feature.melspectrogram(x,sr=fs,n_fft=4096, hop_length=441,n_mels=80, fmin=27.5, fmax=16000)\n",
        "\n",
        "  #log scaling\n",
        "  melgram1=10*np.log10(1e-10+melgram1)\n",
        "  melgram2=10*np.log10(1e-10+melgram2)\n",
        "  melgram3=10*np.log10(1e-10+melgram3)\n",
        "\n",
        "  #normalize\n",
        "  melgram1=(melgram1-np.atleast_2d(means[0]).T)/np.atleast_2d(stds[0]).T\n",
        "  melgram2=(melgram2-np.atleast_2d(means[1]).T)/np.atleast_2d(stds[1]).T\n",
        "  melgram3=(melgram3-np.atleast_2d(means[2]).T)/np.atleast_2d(stds[2]).T\n",
        "\n",
        "  #zero pad ends\n",
        "  melgram1=zeropad2d(melgram1,contextlen)\n",
        "  melgram2=zeropad2d(melgram2,contextlen)\n",
        "  melgram3=zeropad2d(melgram3,contextlen)\n",
        "\n",
        "  #make chunks\n",
        "  melgram1_chunks=makechunks(melgram1,duration)\n",
        "  melgram2_chunks=makechunks(melgram2,duration)\n",
        "  melgram3_chunks=makechunks(melgram3,duration)\n",
        "\n",
        "  #generate song labels\n",
        "  hop_dur=10e-3\n",
        "  labels=np.zeros(melgram1_chunks.shape[0])\n",
        "  weights=np.ones(melgram1_chunks.shape[0])\n",
        "  idxs=np.array(np.round(onsets/hop_dur),dtype=int)\n",
        "  labels[idxs]=1\n",
        "\n",
        "  #target smearing\n",
        "  labels[idxs-1]=1\n",
        "  labels[idxs+1]=1\n",
        "  weights[idxs-1]=0.25\n",
        "  weights[idxs+1]=0.25\n",
        "  labels_dict={}\n",
        "  weights_dict={}\n",
        "\n",
        "  #save\n",
        "  savedir=os.path.join(save_dir,item)\n",
        "  if not os.path.exists(savedir): os.makedirs(savedir)\n",
        "\n",
        "  #runs through each mel spectrum segement for the current song\n",
        "  for i_chunk in range(melgram1_chunks.shape[0]):\n",
        "\n",
        "    #reshapes segments for concatenation\n",
        "    melgram1_reshape = np.reshape(melgram1_chunks[i_chunk],(80,15,1))\n",
        "    melgram2_reshape = np.reshape(melgram2_chunks[i_chunk],(80,15,1))\n",
        "    melgram3_reshape = np.reshape(melgram3_chunks[i_chunk],(80,15,1))\n",
        "\n",
        "    #creates the savepath\n",
        "    savepath=os.path.join(savedir,str(i_chunk)+'.pt')\n",
        "\n",
        "    #concatenates and saves the chunks\n",
        "    save_melgram = np.concatenate((melgram1_reshape,melgram2_reshape,melgram3_reshape), axis=2)\n",
        "    all_data[idx] = save_melgram\n",
        "    all_data_dict[savepath] = idx\n",
        "    np.save(savepath, save_melgram)\n",
        "\n",
        "    #appends the names of the chunks to the labels and weights dictionaries\n",
        "    filelist.append(savepath)\n",
        "    labels_dict[savepath]=labels[i_chunk]\n",
        "    weights_dict[savepath]=weights[i_chunk]\n",
        "    idx += 1\n",
        "\n",
        "  #append labels to master\n",
        "  labels_master.update(labels_dict)\n",
        "  weights_master.update(weights_dict)\n",
        "\n",
        "#saves the final files\n",
        "np.save('/content/drive/MyDrive/Final_data/labels_master',labels_master)\n",
        "np.save('/content/drive/MyDrive/Sythn_data/weights_master',weights_master)\n",
        "np.save('/content/drive/MyDrive/Sythn_data/all_data_dict',all_data_dict)\n",
        "\n",
        "#saves the total data list\n",
        "with h5py.File('/content/drive/MyDrive/Sythn_data/all_data.h5', 'w') as hf:\n",
        "    hf.create_dataset(\"input_data\",  data=all_data)\n",
        "    hf.close"
      ]
    }
  ]
}